
# See LICENSE for license details.

# This file is automatically generated. Do not edit.

#*****************************************************************************
# vxor.vv_LMUL1SEW64.S
#-----------------------------------------------------------------------------
#
# Test vxor.vv insnructions.
# With LMUL=1, SEW=64
#

#include "riscv_test.h"
#include "test_macros.h"

RVTEST_RV64UV

RVTEST_CODE_BEGIN


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  TEST_CASE(2, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(3, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 2
  vsetvli t1, t0, e64,m1,tu,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(6, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(7, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(10, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(11, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(14, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(15, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(18, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(19, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 3
  vsetvli t1, t0, e64,m1,tu,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(22, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(23, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(26, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(27, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(30, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(31, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(34, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(35, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 4
  vsetvli t1, t0, e64,m1,tu,ma
  vxor.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(38, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(39, t0, 0x1, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(42, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(43, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vxor.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)


  TEST_CASE(46, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)
  TEST_CASE(47, t0, 0x0, ld t0, 0(a1); addi a1, a1, 8)




  TEST_PASSFAIL

RVTEST_CODE_END

  .data
RVTEST_DATA_BEGIN

res:
  .zero 144

tdat:
  .quad 0x0
  .quad 0x0
  .quad 0x1
  .quad 0x1
  .quad 0x3
  .quad 0x7
  .quad 0xfffffffffffffff8
  .quad 0x0
  .quad 0xffffffffffffffff

mask:
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555

RVTEST_DATA_END
